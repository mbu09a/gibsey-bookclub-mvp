# 06\_daily\_dag.md â€“ **Nightly Knowledge Distillation**

> **SprintÂ 2Â â€“ BlockÂ 8â€“9Â h**Â Â |Â Â AuthorÂ âœï¸Â ***YouÂ +Â Gemini***
>
> **Goal:** A reproducible **ApacheÂ Airflow** DAG that clusters the latest `page_vectors`, generates 250â€‘word abstractive summaries with LLamaâ€‘3â€‘8B, and writes them to the `summaries` table every night atÂ 03:00Â local. Summaries seed higherâ€‘level context for long conversations and future chapter views.

---

## 1â€‚Rationale

| Problem                                            | Nightly DAG Benefit                                                                  |
| -------------------------------------------------- | ------------------------------------------------------------------------------------ |
| Raw pages are too granular for multiâ€‘page queries. | Provides midâ€‘sized â€œchapterâ€‘likeâ€ chunks (â‰¤250â€¯w) tuned to emergent vector clusters. |
| Onâ€‘demand clustering is compute heavy.             | Batch once nightly while traffic is low.                                             |
| Weâ€™ll soon need *secondary* context for DreamRIA.  | Summaries table doubles as feed for secondary AI personas.                           |

---

## 2â€‚Highâ€‘Level Flow

```mermaid
flowchart TD
  subgraph Batch
    E[page_vectors] -->|pull| DAG[Airflow DAG]
    DAG --> CLU[HDBSCAN/KMeans]
    CLU --> LLM[LLamaâ€‘3â€‘8B Summarize]
    LLM --> S[summaries]
    S --> UI[Vault / Chapter View]
  end
```

---

## 3â€‚Schema: `summaries`

```sql
CREATE TABLE summaries (
  cluster_id int PRIMARY KEY,
  center_page text,
  summary text,
  updated_at timestamp
);
```

*`cluster_id`* is deterministic (stable Kâ€‘Means seed) so downâ€‘stream caches persist.

---

## 4â€‚Airflow Setup

* **Image:** `apache/airflow:2.9-python3.11` (Docker compose).
* **Connections:**

  * `stargate_rest`Â â†’ HTTP (baseÂ `http://stargate:8080`, extra:Â `{"token":"$STARGATE_AUTH_TOKEN"}`)
  * `ollama_chat`Â Â â†’ HTTP (baseÂ `http://ollama:11434`)
* **Env:** `AIRFLOW_UID=$(id -u)` so local volume permissions work.

### 4.1Â dockerâ€‘compose.airflow\.yml (excerpt)

```yaml
services:
  airflow:
    image: apache/airflow:2.9-python3.11
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
    ports: ["8088:8080"]
    depends_on: [stargate, ollama]
```

---

## 5â€‚DAG CodeÂ (`dags/nightly_summarizer.py`)

```python
from airflow import DAG
from airflow.decorators import task
from airflow.utils.dates import days_ago
from pendulum import timezone
import httpx, os, numpy as np, json, hashlib, random
from sklearn.cluster import KMeans

SG   = os.getenv("STARGATE_URL", "http://stargate:8080")
TOKEN= os.getenv("STARGATE_AUTH_TOKEN")
OLL  = os.getenv("OLLAMA_URL", "http://ollama:11434/api/chat")
TZ   = timezone("America/Chicago")

def stargate_get(path):
    return httpx.get(f"{SG}{path}", headers={"X-Cassandra-Token":TOKEN}).json()

def stargate_put(path, body):
    return httpx.put(f"{SG}{path}", json=body, headers={"X-Cassandra-Token":TOKEN})

def summarize_txt(txt):
    payload={"model":"llama3:8b","stream":False,
             "prompt":f"Summarize in 250 words max:\n{txt}"}
    return httpx.post(OLL, json=payload, timeout=120).json()["response"].strip()

def cluster(vecs, k=48):
    km = KMeans(n_clusters=k, random_state=42).fit(vecs)
    return km.labels_, km.cluster_centers_

def representative_page(vecs, labels, pages):
    rep = {}
    for idx,l in enumerate(labels):
        if l not in rep:
            rep[l]=pages[idx]
    return rep

with DAG(
    "nightly_summarizer",
    start_date=days_ago(1),
    schedule_interval="0 3 * * *",
    catchup=False,
    max_active_runs=1,
    tags=["gibsey"],
    default_args={"retries":1}
) as dag:

    @task()
    def pull_vectors():
        data = stargate_get("/v2/keyspaces/gibsey/page_vectors?page-size=1000")
        vecs, pages = [], []
        for row in data["data"]:
            pages.append(row["page_id"])
            vecs.append(row["vector"])
        return {"vecs":vecs, "pages":pages}

    @task()
    def cluster_and_summarize(payload):
        vecs = np.array(payload["vecs"], dtype="float32")
        pages= payload["pages"]
        labels, _ = cluster(vecs, k=48)
        rep     = representative_page(vecs, labels, pages)
        batch   = {}
        for l,pid in rep.items():
            body = stargate_get(f"/v2/keyspaces/gibsey/pages/{pid}")["body"]
            batch[l] = {
                "center_page": pid,
                "summary": summarize_txt(body)
            }
        return batch

    @task()
    def write_summaries(batch):
        ts = int(__import__("time").time()*1000)
        for cid,data in batch.items():
            row = {
                "cluster_id": int(cid),
                "center_page": data["center_page"],
                "summary": data["summary"],
                "updated_at": ts
            }
            stargate_put(f"/v2/keyspaces/gibsey/summaries/{cid}", row)

    write_summaries(cluster_and_summarize(pull_vectors()))
```

---

## 6â€‚Performance Targets

| Stage           | Budget                                   |
| --------------- | ---------------------------------------- |
| Vector pull     | <Â 20â€¯s                                   |
| Clustering (48) | <Â 5â€¯s on 700 vectors                     |
| Summary LLM     | â‰¤Â 15â€¯s each â†’ parallel loop â†’ total 60â€¯s |
| Stargate writes | <Â 10â€¯s                                   |
| Total DAG time  | **â‰¤Â 2â€¯minÂ 30â€¯s**                         |

---

## 7â€‚Monitoring & Alerting

* **Airflow UI** â†’ DAG duration SLAÂ `<150Â s`.
* **Prometheus**: scrape taskâ€‘duration metric, alert if `duration_seconds{task="summarize"} > 30`.
* **Retry**: Airflow autoâ€‘retry once; second failure sends email/Slack.

---

## 8â€‚LocalÂ Test

```bash
docker compose -f docker-compose.airflow.yml up -d airflow
# In UI trigger DAG run manually
```

Check `summaries` table afterward:

```bash
curl $SG/v2/keyspaces/gibsey/summaries | jq '.data[0]'
```

---

## 9â€‚Troubleshooting

| Issue                           | Check                      | Fix                                                      |
| ------------------------------- | -------------------------- | -------------------------------------------------------- |
| Â `ModuleNotFoundError: sklearn` | Image missing dependencies | Add `scikit-learn==1.5` to `requirements.txt`.           |
| Empty summaries                 | LLM hit context window     | Ensure `body` â‰¤Â 2â€¯000Â tokens or summarize incrementally. |
| DAG stuck in running            | LLM streaming never closes | Use `stream=False` payload; set HTTP timeout 120â€¯s.      |

---

## 10â€‚DeliveryÂ Checklist (45Â min)

* [ ] Airflow compose up & metadatabase initialized.
* [ ] DAG file copied â†’ container volume.
* [ ] Env connections set.
* [ ] Manual run completes <Â 3Â min.
* [ ] Prometheus metric appears.
* [ ] Commit/tagÂ `dag-ready`.

---

## 11â€‚Handâ€‘Off

> â€œNightly summarizer operational. Final block `07_smoke_tests.md`: capture p95 latency, RAG miss rate, SSE firstâ€‘token time in CI.â€

*End of file ğŸŒ’*
