# 03\_faust\_worker.md â€“ **Streaming Embeddings Pipeline**

> **SprintÂ 2 â€“ BlockÂ 2â€“4Â h**  |  Author âœï¸ ***YouÂ +Â Gemini***
>
> **Goal:** A Faust application that consumes Cassandraâ€‘CDC events from Kafka, generates 768â€‘dimensional embeddings with **Ollamaâ†’`nomic-embed-text`**, upserts into `page_vectors` via Stargate, **and** notifies the RAG service in <â€¯5â€¯s.

---

## 1â€‚Why Faust?

| Need                                                 | Faust Advantage                            |
| ---------------------------------------------------- | ------------------------------------------ |
| Pythonic streamâ€‘processing; zero JVM boilerplate     | Native `async` coroutines, JSON typing     |
| Hotâ€‘reload for rapid schema tweaks                   | `faust -A app worker -l info --autoreload` |
| Exactlyâ€‘once semantics option via Kafka transactions | Safe idempotent upserts                    |

---

## 2â€‚Prerequisites

* **BlocksÂ 01 & 02** running â€“ Kafka broker `kafka:9092`, Stargate env vars.
* Local **Ollama** container exposing `http://ollama:11434/api/embeddings`.
* Topic **`cdc.pages`** exists (Debezium default) with JSON rows like:

  ```json
  { "page_id":"42", "body":"â€¦", "op":"c" }
  ```
* Docker Desktop â‰¥â€¯24 or Compose v2.

---

## 3â€‚Directory Layout

```
faust_worker/
  â”œâ”€ app.py              # main agent
  â”œâ”€ Dockerfile
  â”œâ”€ requirements.txt
  â”œâ”€ embed_client.py     # wrapper for Ollama & fallback
  â””â”€ tests/
      â””â”€ test_embed.py
```

---

## 4â€‚requirements.txt

```txt
faust-streaming==0.9.12   # official rewrite of faust
aiohttp==3.9.5            # HTTP client for Stargate & Ollama
python-dotenv==1.0.1      # env injection in dev
```

---

## 5â€‚Dockerfile

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
ENV PYTHONUNBUFFERED=1
CMD ["faust", "-A", "app", "worker", "-l", "info", "--web-port", "6066"]
```

> **Note:** PortÂ 6066 exposes Faust web/metrics UI (`/metrics`, Prometheusâ€‘ready).

---

## 6â€‚Core Code â€“ app.py

```python
import faust, os, aiohttp, logging, json
from embed_client import embed_text

STARGATE   = os.getenv("STARGATE_URL", "http://stargate:8080")
TOKEN      = os.getenv("STARGATE_AUTH_TOKEN")
RAG_HOOK   = os.getenv("RAG_REFRESH_URL", "http://rag_service:8001/refresh")
TOPIC      = os.getenv("KAFKA_TOPIC", "cdc.pages")

class Row(faust.Record, serializer="json"):
    page_id: str
    body: str
    op: str   # c = create, u = update

app = faust.App(
    "gibsey-embedder",
    broker="kafka://kafka:9092",
    topic_partitions=3,
)
stream = app.topic(TOPIC, value_type=Row)
http   = aiohttp.ClientSession()

async def stargate_upsert(page_id: str, vec: list[float]):
    url = f"{STARGATE}/v2/keyspaces/gibsey/page_vectors/{page_id}"
    headers = {"X-Cassandra-Token": TOKEN, "Content-Type": "application/json"}
    payload = {"vector": vec}
    async with http.put(url, json=payload, headers=headers) as r:
        if r.status not in (200, 201):
            logging.error("Stargate upsert failed %s", await r.text())

async def rag_notify(page_id: str, vec: list[float]):
    payload = {"page_id": page_id, "vector": vec}
    try:
        async with http.post(RAG_HOOK, json=payload, timeout=5):
            pass
    except Exception as e:
        logging.warning("RAG hook failed: %s", e)

@app.agent(stream)
async def process(rows):
    async for row in rows.filter(lambda r: r.op in ("c", "u")):
        vec = await embed_text(row.body)
        await stargate_upsert(row.page_id, vec)
        await rag_notify(row.page_id, vec)
```

### 6.1â€‚embed\_client.py

```python
import aiohttp, os, logging, json, hashlib

EMBED_URL = os.getenv("EMBED_URL", "http://ollama:11434/api/embeddings")
CACHE     = {}

async def embed_text(text: str) -> list[float]:
    key = hashlib.md5(text.encode()).hexdigest()
    if key in CACHE:
        return CACHE[key]
    async with aiohttp.ClientSession() as s:
        async with s.post(EMBED_URL, json={"model":"nomic-embed-text","prompt":text}) as r:
            if r.status != 200:
                logging.error("Embed API fail %s", await r.text()); return []
            vec = (await r.json())["embedding"]
            CACHE[key] = vec
            return vec
```

---

## 7â€‚Local Dev

```bash
cd faust_worker
pip install -r requirements.txt
export STARGATE_URL=http://localhost:8080
export STARGATE_AUTH_TOKEN=dev-super-secret
python -m faust -A app worker -l info --autoreload
```

Insert a test page via Stargate; log should show `Upsert OK`.

---

## 8â€‚Compose Service Snippet

```yaml
  faust_worker:
    build: ./faust_worker
    environment:
      - KAFKA_BROKER=kafka:9092
      - STARGATE_URL=http://stargate:8080
      - STARGATE_AUTH_TOKEN=${STARGATE_AUTH_TOKEN}
      - RAG_REFRESH_URL=http://rag_service:8001/refresh
    depends_on: [kafka, stargate, ollama]
```

Bring up just the worker:

```bash
docker compose up -d faust_worker
```

---

## 9â€‚Scaling & Performance Tips

| Scenario                     | Action                                                                                                |
| ---------------------------- | ----------------------------------------------------------------------------------------------------- |
| Throughput > 200 msgs/s      | Increase Kafka partitions to 6; set `topic_partitions=6`; run 2 worker replicas.                      |
| Embedding latency bottleneck | Run **GPU Ollama** image or switch to `text-embedding-3-small` via OpenAI for speed (cost tradeâ€‘off). |
| Backâ€‘pressure on Stargate    | Batch vectors: collect 100, then bulk PUT `/page_vectors` custom endpoint.                            |

---

## 10â€‚Monitoring & Alerting

* **Faust Web UI:** `http://localhost:6066/` â†’ lag, tasks.
* **Prometheus:** scrape `faust_worker:6066/metrics`.
* **Alert:** page\_vectors updates <â€¯100/min â†’ Slack webhook.

---

## 11â€‚Testing

```bash
pytest tests/ -q
```

Sample test: `test_embed.py` mocks Ollama endpoint, verifies 768â€‘length vector.

---

## 12â€‚Troubleshooting

| Symptom                            | Likely Cause                                          | Fix                                                  |
| ---------------------------------- | ----------------------------------------------------- | ---------------------------------------------------- |
| `ValueError: SSL handshake failed` | Kafka TLS misâ€‘config (local plaintext)                | Ensure broker URL `kafka://` not `kafka+ssl://`      |
| Worker memory climb                | Embedding cache too large                             | Limit CACHE size (`functools.lru_cache(maxsize=2k)`) |
| Duplicate vector rows              | Faiss worker running twice but topic only 1 partition | Match worker replicas to partitions.                 |

---

## 13â€‚Delivery Checklist (90â€¯min target)

* [ ] Write `faust_worker/` files (Gemini) â€“ 20â€¯min
* [ ] Build & run container â€“ 10â€¯min pull
* [ ] Insert smoke page â†’ vector row appears â€“ 5â€¯min
* [ ] RAG service receives `/refresh` â€“ 5â€¯min
* [ ] Commit, tag `faust-ok`, push â€“ 5â€¯min

Total â‰ˆ **45â€¯min handsâ€‘on**, buffer for debug.

---

## 14â€‚Handâ€‘Off to Next Block

> â€œFaust worker streaming embeddings confirmed. Begin `04_memory_rag.md`; assume `POST /refresh` delivers `{page_id, vector}` payload within 2â€¯s of inserts.â€

*End of file ğŸ§¬*
