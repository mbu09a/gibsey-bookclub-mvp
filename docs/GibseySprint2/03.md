# 03\_faust\_worker.md – **Streaming Embeddings Pipeline**

> **Sprint 2 – Block 2–4 h**  |  Author ✍️ ***You + Gemini***
>
> **Goal:** A Faust application that consumes Cassandra‑CDC events from Kafka, generates 768‑dimensional embeddings with **Ollama→`nomic-embed-text`**, upserts into `page_vectors` via Stargate, **and** notifies the RAG service in < 5 s.

---

## 1 Why Faust?

| Need                                                 | Faust Advantage                            |
| ---------------------------------------------------- | ------------------------------------------ |
| Pythonic stream‑processing; zero JVM boilerplate     | Native `async` coroutines, JSON typing     |
| Hot‑reload for rapid schema tweaks                   | `faust -A app worker -l info --autoreload` |
| Exactly‑once semantics option via Kafka transactions | Safe idempotent upserts                    |

---

## 2 Prerequisites

* **Blocks 01 & 02** running – Kafka broker `kafka:9092`, Stargate env vars.
* Local **Ollama** container exposing `http://ollama:11434/api/embeddings`.
* Topic **`cdc.pages`** exists (Debezium default) with JSON rows like:

  ```json
  { "page_id":"42", "body":"…", "op":"c" }
  ```
* Docker Desktop ≥ 24 or Compose v2.

---

## 3 Directory Layout

```
faust_worker/
  ├─ app.py              # main agent
  ├─ Dockerfile
  ├─ requirements.txt
  ├─ embed_client.py     # wrapper for Ollama & fallback
  └─ tests/
      └─ test_embed.py
```

---

## 4 requirements.txt

```txt
faust-streaming==0.9.12   # official rewrite of faust
aiohttp==3.9.5            # HTTP client for Stargate & Ollama
python-dotenv==1.0.1      # env injection in dev
```

---

## 5 Dockerfile

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
ENV PYTHONUNBUFFERED=1
CMD ["faust", "-A", "app", "worker", "-l", "info", "--web-port", "6066"]
```

> **Note:** Port 6066 exposes Faust web/metrics UI (`/metrics`, Prometheus‑ready).

---

## 6 Core Code – app.py

```python
import faust, os, aiohttp, logging, json
from embed_client import embed_text

STARGATE   = os.getenv("STARGATE_URL", "http://stargate:8080")
TOKEN      = os.getenv("STARGATE_AUTH_TOKEN")
RAG_HOOK   = os.getenv("RAG_REFRESH_URL", "http://rag_service:8001/refresh")
TOPIC      = os.getenv("KAFKA_TOPIC", "cdc.pages")

class Row(faust.Record, serializer="json"):
    page_id: str
    body: str
    op: str   # c = create, u = update

app = faust.App(
    "gibsey-embedder",
    broker="kafka://kafka:9092",
    topic_partitions=3,
)
stream = app.topic(TOPIC, value_type=Row)
http   = aiohttp.ClientSession()

async def stargate_upsert(page_id: str, vec: list[float]):
    url = f"{STARGATE}/v2/keyspaces/gibsey/page_vectors/{page_id}"
    headers = {"X-Cassandra-Token": TOKEN, "Content-Type": "application/json"}
    payload = {"vector": vec}
    async with http.put(url, json=payload, headers=headers) as r:
        if r.status not in (200, 201):
            logging.error("Stargate upsert failed %s", await r.text())

async def rag_notify(page_id: str, vec: list[float]):
    payload = {"page_id": page_id, "vector": vec}
    try:
        async with http.post(RAG_HOOK, json=payload, timeout=5):
            pass
    except Exception as e:
        logging.warning("RAG hook failed: %s", e)

@app.agent(stream)
async def process(rows):
    async for row in rows.filter(lambda r: r.op in ("c", "u")):
        vec = await embed_text(row.body)
        await stargate_upsert(row.page_id, vec)
        await rag_notify(row.page_id, vec)
```

### 6.1 embed\_client.py

```python
import aiohttp, os, logging, json, hashlib

EMBED_URL = os.getenv("EMBED_URL", "http://ollama:11434/api/embeddings")
CACHE     = {}

async def embed_text(text: str) -> list[float]:
    key = hashlib.md5(text.encode()).hexdigest()
    if key in CACHE:
        return CACHE[key]
    async with aiohttp.ClientSession() as s:
        async with s.post(EMBED_URL, json={"model":"nomic-embed-text","prompt":text}) as r:
            if r.status != 200:
                logging.error("Embed API fail %s", await r.text()); return []
            vec = (await r.json())["embedding"]
            CACHE[key] = vec
            return vec
```

---

## 7 Local Dev

```bash
cd faust_worker
pip install -r requirements.txt
export STARGATE_URL=http://localhost:8080
export STARGATE_AUTH_TOKEN=dev-super-secret
python -m faust -A app worker -l info --autoreload
```

Insert a test page via Stargate; log should show `Upsert OK`.

---

## 8 Compose Service Snippet

```yaml
  faust_worker:
    build: ./faust_worker
    environment:
      - KAFKA_BROKER=kafka:9092
      - STARGATE_URL=http://stargate:8080
      - STARGATE_AUTH_TOKEN=${STARGATE_AUTH_TOKEN}
      - RAG_REFRESH_URL=http://rag_service:8001/refresh
    depends_on: [kafka, stargate, ollama]
```

Bring up just the worker:

```bash
docker compose up -d faust_worker
```

---

## 9 Scaling & Performance Tips

| Scenario                     | Action                                                                                                |
| ---------------------------- | ----------------------------------------------------------------------------------------------------- |
| Throughput > 200 msgs/s      | Increase Kafka partitions to 6; set `topic_partitions=6`; run 2 worker replicas.                      |
| Embedding latency bottleneck | Run **GPU Ollama** image or switch to `text-embedding-3-small` via OpenAI for speed (cost trade‑off). |
| Back‑pressure on Stargate    | Batch vectors: collect 100, then bulk PUT `/page_vectors` custom endpoint.                            |

---

## 10 Monitoring & Alerting

* **Faust Web UI:** `http://localhost:6066/` → lag, tasks.
* **Prometheus:** scrape `faust_worker:6066/metrics`.
* **Alert:** page\_vectors updates < 100/min → Slack webhook.

---

## 11 Testing

```bash
pytest tests/ -q
```

Sample test: `test_embed.py` mocks Ollama endpoint, verifies 768‑length vector.

---

## 12 Troubleshooting

| Symptom                            | Likely Cause                                          | Fix                                                  |
| ---------------------------------- | ----------------------------------------------------- | ---------------------------------------------------- |
| `ValueError: SSL handshake failed` | Kafka TLS mis‑config (local plaintext)                | Ensure broker URL `kafka://` not `kafka+ssl://`      |
| Worker memory climb                | Embedding cache too large                             | Limit CACHE size (`functools.lru_cache(maxsize=2k)`) |
| Duplicate vector rows              | Faiss worker running twice but topic only 1 partition | Match worker replicas to partitions.                 |

---

## 13 Delivery Checklist (90 min target)

* [ ] Write `faust_worker/` files (Gemini) – 20 min
* [ ] Build & run container – 10 min pull
* [ ] Insert smoke page → vector row appears – 5 min
* [ ] RAG service receives `/refresh` – 5 min
* [ ] Commit, tag `faust-ok`, push – 5 min

Total ≈ **45 min hands‑on**, buffer for debug.

---

## 14 Hand‑Off to Next Block

> “Faust worker streaming embeddings confirmed. Begin `04_memory_rag.md`; assume `POST /refresh` delivers `{page_id, vector}` payload within 2 s of inserts.”

*End of file 🧬*
