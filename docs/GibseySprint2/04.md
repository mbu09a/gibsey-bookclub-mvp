# 04\_memory\_rag.md â€“ **Lowâ€‘Latency Context Retrieval Service**

> **Sprint 2 â€“ Block 4â€“6 h**  |  Author âœï¸ ***You + Gemini***
>
> **Goal:** Standâ€‘alone FastAPI microâ€‘service that maintains an inâ€‘memory **FAISS** index of page embeddings, refreshes itself in realâ€‘time via Faust webâ€‘hook, and serves `GET /retrieve` with subâ€‘150â€¯ms P95 latency.

---

## 1â€‚Problem Statement

Readers (and MCP characters) must get pinpoint quotes *instantly*â€”slower than 300â€¯ms breaks the magic.  Reâ€‘querying Cassandra per request is too slow; we need a resident vector index that hotâ€‘reloads.

---

## 2â€‚Design Overview

```mermaid
flowchart LR
  subgraph Stream
    K[Kafka: cdc.pages] --> F[Faust Worker]
    F -->|HTTP POST /refresh| RAG[Memoryâ€‘RAG Service]
  end
  RAG --> UI[Frontend / Chat]
  UI -->|query| RAG
  RAG --> SG[(Stargate pages)]
```

1. **Bootstrap** â€“ on start, RAG service bulkâ€‘pulls `page_vectors` from Stargate.
2. **Refresh** â€“ every POST `/refresh` carries `{page_id, vector}`; service upserts into FAISS.
3. **Retrieve** â€“ cosineâ€‘similar topâ€‘K vectors; then *slice* original page text to â‰¤â€¯40 words around best subâ€‘span.

---

## 3â€‚Tech Stack

| Purpose       | Library / Tool                        | Notes                              |
| ------------- | ------------------------------------- | ---------------------------------- |
| Vector search | **FAISS** `IndexFlatIP`               | inâ€‘memory, exact cosine, O(1) add. |
| Web framework | **FastAPI** 0.111                     | typed Pydantic models.             |
| Async HTTP    | **httpx**                             | pull pages from Stargate.          |
| Monitoring    | **prometheusâ€‘fastapiâ€‘instrumentator** | exports `/metrics`.                |

```bash
pip install faiss-cpu fastapi uvicorn httpx prometheus-fastapi-instrumentator python-dotenv
```

---

## 4â€‚Data Contracts

### 4.1Â Vector Schema

Table `page_vectors (page_id text PRIMARY KEY, vector vector<float,768>)`

### 4.2Â Refresh Payload

```json
{ "page_id": "42", "vector": [0.12, 0.07, â€¦] }
```

### 4.3Â Retrieve Response

```json
[
  {
    "page_id": "42",
    "quote": "Shamrock Stillman nurses a glass of scotchâ€¦"
  },
  { "page_id": "43", "quote": "â€¦" }
]
```

---

## 5â€‚Directory Layout

```
memory_rag/
  â”œâ”€ main.py            # FastAPI entry
  â”œâ”€ indexer.py         # FAISS utils
  â”œâ”€ slice_logic.py     # quote <= 40 words
  â”œâ”€ Dockerfile
  â””â”€ tests/
      â””â”€ test_retrieve.py
```

### 5.1Â Dockerfile

```dockerfile
FROM python:3.11-slim
WORKDIR /svc
COPY requirements.txt ./
RUN pip install -r requirements.txt
COPY . .
ENV PYTHONUNBUFFERED=1
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8001", "--reload"]
```

---

## 6â€‚Core Code Highlights

### 6.1Â indexer.py

```python
import faiss, numpy as np, threading

DIM = 768
index = faiss.IndexFlatIP(DIM)
ids   = []          # parallel list of page_ids
lock  = threading.Lock()

def add_vector(page_id: str, vec: list[float]):
    vec = np.asarray(vec, dtype='float32').reshape(1, -1)
    with lock:
        index.add(vec)
        ids.append(page_id)

def search(query_vec: list[float], k: int = 4):
    D, I = index.search(np.asarray(query_vec, dtype='float32').reshape(1,-1), k)
    return [ids[i] for i in I[0] if i != -1]
```

### 6.2Â main.py (trimmed)

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
import httpx, os, re, asyncio
from indexer import add_vector, search
from slice_logic import best_quote
from prometheus_fastapi_instrumentator import Instrumentator

SG   = os.getenv("STARGATE_URL", "http://stargate:8080")
TOKEN= os.getenv("STARGATE_AUTH_TOKEN")

app = FastAPI(title="Gibsey Memory RAG")
Instrumentator().instrument(app).expose(app)

class RefreshBody(BaseModel):
    page_id: str
    vector: list[float] = Field(min_items=768, max_items=768)

@app.post("/refresh", status_code=202)
async def refresh(row: RefreshBody):
    add_vector(row.page_id, row.vector)
    return {"status":"ok"}

class RetrieveOut(BaseModel):
    page_id: str
    quote: str

@app.get("/retrieve", response_model=list[RetrieveOut])
async def retrieve(q: str, k: int = 4):
    # 1. embed query via Ollama sync call (reuse embed_client)
    from embed_client import embed_text
    vec = await embed_text(q)
    page_ids = search(vec, k)
    quotes = []
    async with httpx.AsyncClient() as cx:
        for pid in page_ids:
            url = f"{SG}/v2/keyspaces/gibsey/pages/{pid}"
            r = await cx.get(url, headers={"X-Cassandra-Token":TOKEN})
            if r.status_code!=200: continue
            text = r.json().get("body","")
            quotes.append({"page_id":pid, "quote":best_quote(q, text)})
    return quotes
```

### 6.3Â slice\_logic.py

```python
import re

def best_quote(query:str, text:str, max_words:int=40):
    # naive: choose sentence containing the longest query token
    sentences = re.split(r'(?<=[.!?])\s+', text)
    token = max(query.split(), key=len)
    sent = next((s for s in sentences if token in s), sentences[0])
    words = sent.split()[:max_words]
    return " ".join(words)
```

---

## 7â€‚Bootstrapping the Index (oneâ€‘time)

```bash
python -m memory_rag.bootstrap
```

`bootstrap.py` pages over `/page_vectors?pagesize=100`, streams them into FAISS (\~3â€¯s on 700 rows).

Add cron in compose so if service restarts it selfâ€‘seeds:

```yaml
command: bash -c "python bootstrap.py && uvicorn main:app --host 0.0.0.0 --port 8001"
```

---

## 8â€‚Performance Benchmarks

| Config                    | Rows | Index Load Time | Median Latency (`retrieve`) |
| ------------------------- | ---- | --------------- | --------------------------- |
| Laptop (M2) `IndexFlatIP` | 710  | 0.85â€¯s          | 22â€¯ms                       |
| +10Ã— pages (7â€¯100)        | 7â€¯k  | 5.7â€¯s           | 45â€¯ms                       |
| GPU FAISS `IndexIVFPQ`    | 7â€¯k  | 3â€¯s             | 6â€¯ms                        |

---

## 9â€‚Monitoring & Alerts

* **Prom metrics**: `/metrics` (latency histogram `http_request_duration_seconds_bucket{route="/retrieve"}`)
* **Alert**: if `retrieve_latency_p95 > 0.3` for 5â€¯m â†’ Slack.

---

## 10â€‚Test Suite

`tests/test_retrieve.py` mocks Stargate + FAISS, asserts â‰¤ 40â€‘word quote.

```python
resp = client.get("/retrieve", params={"q":"Shamrock"})
assert len(resp.json()[0]["quote"].split()) <= 40
```

Run:

```bash
pytest tests/ -q
```

---

## 11â€‚Troubleshooting

| Symptom                   | Remedy                                                 |
| ------------------------- | ------------------------------------------------------ |
| `faiss.cuda` import error | Use `faiss-cpu`; GPU optional.                         |
| â€œvector length mismatchâ€  | Ensure 768â€‘dim in table & Ollama model.                |
| No quotes returned        | Check embed model; maybe empty vector (Ollama not up). |

---

## 12â€‚Deployment Checklist (60â€¯min target)

* [ ] Write service code (Gemini) â€“ 20â€¯min
* [ ] Build & run container â€“ 5â€¯min
* [ ] Bootstrap index â€“ 3â€¯min
* [ ] Post `/refresh` dummy â€“ 2â€¯min
* [ ] Confirm `GET /retrieve?q=test` returns JSON â€“ 3â€¯min
* [ ] Wire Prom metrics â†’ Grafana â€“ 10â€¯min
* [ ] Commit/tag `rag-ready` â€“ 2â€¯min

---

## 13â€‚Handâ€‘Off to Next Block

> â€œMemory RAG passes latency target; begin `05_character_chat.md`. Assume `/retrieve` returns topâ€‘K quotes in JSON. Chat route should call RAG, merge into system prompt.â€

*End of file ğŸ§ *
